Introduction

Recommender system is an artificially designed system to mine users' interest based on historical data, helping decision making, booming sales and achieving better user experience. It is born to tackle information overload under current Big Data environment.

Back in old days, there are mainly two kinds of websites: web portals and search engine. 

A web portal is most often one specially-designed Web page at a website which brings information together from diverse sources in a uniform way (definition from http://en.wikipedia.org/wiki/Web_portal). For example, Yahoo, one of the leading members in this area, provides precisely classified information to every visitor. The problem with this type of website is that it is very hard to artificially position every information, for some of them may belong to various areas, while others may be hard to classify when evaluating from different views. Web editors' mind may not always match everybody's thought, which may annoy some users. Another problem with portals is that it can mainly list the most important and popular events and exclude many "trivial" information, making the diversity of information poor. 

A web search engine is a software system that is designed to search for information on the World Wide Web (definitions from http://en.wikipedia.org/wiki/Search_engine). Different from portals, the GUI of a web search engine, like Google, is quite neat, with only a textbox waiting for users' input. Its idea is to provide the most relevant information according to user needs, a kind of personalized service. However, the main shortage here is that users have to figure out what they really want and make rather precise description (usually in natural language) about desired information. In many cases, people could not make both conditions matched. For example, when surfing on a music fm, most people won't be able to give the exact list of music of their tastes and describe precisely the type of music they love. Thus we need to develop a system which can automatically do these things for users and provide personalized service. And this is where Recommender system origins.

Since 1990s, Recommender system has been widely used in various web applications. Douban.fm, an online music radio, uses recommendation algorithms to decide next song based on users' historical likes and dislikes. Amazon, a large e-business company, will recommend new things similar to users' previous shopping records. Facebook, the most popular social website, uses common friends as the main feature to make friends recommendation, help enlarge users' friends circle. Hulu, a newly-risen company, recommends Ads to users based on their tastes, bringing the whole profit to a new level. With all these successful business applications, Recommender System has been playing an increasingly important role in current web environment.

During its application, Recommender System has confronted many real-life problems. The first one is called cold-start problem, where it is very hard to recommend things to newly-registered users, because system knows very little about their own information. Second is called explainability. It is not easy to give a reasonable explanation for each recommendation result, which may wonder users and make it less likely for them to accept. Third comes sparsity problem. In real system, each user will only consume a small part of all the items, which makes the user-item matrix a highly sparse matrix. This  challenges many algorithms in accuracy and speed. Last comes the scalability problem. Many algorithms produced in academic field are hard to be implemented in real systems, for their performance may suffer severely coping with data of Gigabytes. Other issues will not be included in the discussion here.

Evaluation Metrics

There are mainly four metrics to evaluate recommendation algorithms.
1. Recall
Recall is the ratio of "good" recommendation results in all products users consumed in test set. Here a recommendation result could be considered "good" if it has been consumed in test set.   
2. Accuracy
Accuracy is the ratio of "good" ones in all recommendation results.  
3. Coverage
Coverage is the ratio of recommendation results in all potential products, which describes the extend to which the algorithm traverse over all the products.
4. Popularity
Popularity is synthesized indicator describing the average popularity of all the recommendation results. Here one's popularity is weighed by the number of users who consumed this products.

The State-of-the-Art Algorithms

With a development of two decades, many different algorithms have been introduced. The following is a brief conclusion about all the mainstream algorithms in this field. 

Vector Space Model
In this model, users and items are represented by vectors, with each dimension indicating a feature. The favor of this model is that we could bring in linear algebra method, which is a very powerful mathematical tool. 

Graph Model
In this model, users and items are represented by vertexes, while the relationship between users and items can be represented by edges. The favor of this model is that we could bring in Graph theory, another very powerful mathematical tool.

Association Rule
Early researchers tried Data Mining methods to solve the problem in this field. Association Rule Mining is a very popular technique to find fixed patterns from historical data. One of the famous example is "diaper and beer" where analysts found these two things tend to appear at the same time. In Recommender System, researchers used this to find the co-occurence behaviors, like what kind of things does a user tends to buy together, and then recommend things which are not in the shopping basket but quite "similar" to those things already bought.

Content-Based Method
In this model, people will predefine lots of features, like size and color, for items and abstractly represent items with these feature vectors. Then we calculate a similarity rate between each items. For example, we could define the cosine angle between two vectors as an indicator of similarity. Pandora, an online music website, started a project called Music Genome where every piece of music has been analyzed from different aspects, like its type and player, and make recommendations based on these information. This is a direct and successful application of content-based methods.

Collaborate Filtering
Collaborative Filtering (CF) is the process of filtering for information or patterns using techniques involving collaboration among multiple agents (definition from http://en.wikipedia.org/wiki/Collaborative_filtering). The assumption is that people with the same interest will buy similar things. In user-based CF, users are represented by the items they bought while in item-based CF, items are represented by users who bought them. The former one is generally used in scenarios where users tend to hold fixed interest and items update very fast, like a news website. The latter one is commonly used in scenarios where items tend to be fixed and users have wide interest. 

Latent Model
We can also assume the latent structure of the problem and use machine learning method to "learn" the parameters of that model. For example, we can assume that the user-item rating matrix A (where Aij represents the score user i gave to item j) is a multiplication of two matrices U and I where each row in U represents a user and each column in I represents an item. We can use stochastic gradient descent method to train U and I. This method is called Singular Value Decomposition (SVD) and behave very well in Netfilx Movie Recommendation Competition. 

Hybrid Method
A hybrid method is one that combines results generated by several methods to improve the performance of a single recommendation method. In the first step, several recommendation methods will be used to generate different solutions out of the same problem. Then a combiner runs to allocate a weight to each result and then combine (mostly linearly) the results into one final recommendation list. Actually, most recommendation system today uses a hybrid method.
